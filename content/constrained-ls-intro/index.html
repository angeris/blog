<!doctype html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
     <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
     <link rel="stylesheet" href="/libs/highlight/github.min.css">
   
    <link rel="stylesheet" href="/css/franklin.css">
    <link rel="stylesheet" href="/css/basic.css">
    <link rel="icon" href="/assets/favicon.png">
     <title>Least-squares and image processing</title>  
</head>
<body>
    <header>
<div class="blog-name"><a href="/">Longest Path Search</a></div>
<nav>
  <ul>
    <li><a href="https://angeris.github.io">About</a></li>
    <li><a href="https://github.com/angeris">Code</a></li>
    <li><a href="/feed.xml">RSS feed</a></li>
    <li><a href="https://c.xkcd.com/random/comic/">Fun</a></li>
  </ul>
</nav>
</header>

    
    
    <div class="franklin-content">
        <h1>Least-squares and image processing</h1>
        <p>Posted <strong>2017-09-19</strong></p>
    </div>
    
    <!-- Content appended here -->
    
<div class="franklin-content">
<p>Least squares is one of those things that seems relatively simple once you first look at it &#40;perhaps also because most linear algebra texts relegate it to nothing more than a page or two on their textbooks&#41;, but has surprisingly powerful implications that are quite nice, and, most importantly, that are easily computable.</p>
<p><em>If you&#39;re interested in looking at the results first, I&#39;d recommend skipping the following section and going immediately to the next one, which shows the application.</em></p>
<p>So, let&#39;s dive right in&#33;</p>
<h2>Vanilla least squares</h2>
<p>The usual least-squares many of us have heard of is a problem of the form</p>
:&#36;
<p>\min_x \,\,\lVert Ax - b \lVert^2 &#36;</p>
<p>where I define \(\lVert y\lVert^2 \equiv \sum_i y_i^2 = y^Ty\) to be the usual Euclidean norm &#40;and &#36;y^T&#36; denotes the transpose of \(y\)&#41;. This problem has a unique solution provided that \(A\) is full-rank &#40;i.e. has independent columns&#41;, and therefore that \(A^TA\) is invertible.<a href="If &#36;A^TA x &#61; 0&#36; then &#36;x^TA^TAx &#61; 0&#36;, but &#36;x^TA^TAx &#61; &#40;Ax&#41;^T&#40;Ax&#41; &#61; \lVert Ax \lVert^2 &#61; 0&#36; which is zero only when &#36;Ax &#61; 0&#36;. E.g. only if there is an &#36;x&#36; in the nullspace of &#36;A&#36;.">^sq-invertible</a> This is true since the problem above is convex &#40;e.g. any local minimum, if it exists, corresponds to the global minimum<a href="A proof is straightforward. Let&#39;s say &#36;f&#36; is differentiable, since this is the case we care about, then we say &#36;f&#36; is convex if the hyperplane given by &#36;\nabla f&#40;x&#41;&#36; &#40;at any &#36;x&#36;&#41; bounds &#36;f&#36; from below.">^convex-global-min</a>&#41;, coercive &#40;the function diverges to infinity, and therefore <em>has</em> a local minimum&#41; and differentiable such that</p>
\[
\nabla \lVert Ax - b \lVert^2 = \nabla (Ax-b)^T(Ax-b) = 2A^T(Ax-b) = 0,
\]
<p>or that, after rearranging the above equation,</p>
\[
A^TAx = A^Tb.
\]
<p>This equation is called the <em>normal equation</em>, which has a unique solution for \(x\) since we said \(A^TA\) is invertible. In other words, we can write down the &#40;surprisingly, less useful&#41; equation for \(x\)</p>
\[
x = (A^TA)^{-1}A^Tb.
\]
<p>A simple example of direct least squares can be found on the <a href="/pid-ls.html">previous post</a>, but that&#39;s nowhere as interesting as an <em>actual</em> example, using some images, presented below. First to show the presented example is possible, I should note that this formalism can be immediately extended to cover a &#40;multi-&#41;objective problem of the form, for \(\lambda_i > 0\)</p>
\[
\min_x \,\,\lambda_1\lVert A_1x - b_1 \lVert^2 + \lambda_2\lVert A_2x - b_2 \lVert^2 + \dots + \lambda_n\lVert A_nx - b_n \lVert^2
\]
<p>by noting that &#40;say, with two variables, though the idea extends to any number of objectives&#41;, we can pull the \(\lambda_i\) into the inside of the norm and observing that</p>
\[
\lVert a\lVert^2 + \lVert b \lVert^2 = \left \lVert 
\begin{bmatrix}
a\\
b
\end{bmatrix}
\right\lVert^2.
\]
<p>So we can rewrite the above multi-objective problem as</p>
\[
\lambda_1\lVert A_1x - b_1 \lVert^2 + \lambda_2\lVert A_2x - b_2 \lVert^2  = \left\lVert 
\begin{bmatrix}
\sqrt{\lambda_1} A_1\\
\sqrt{\lambda_2} A_2
\end{bmatrix}
x
-
\begin{bmatrix}
\sqrt{\lambda_1}b_1\\
\sqrt{\lambda_2}b_2
\end{bmatrix}\right\lVert^2.
\]
<p>Where the new matrices above are defined as the &#39;stacked&#39; &#40;appended&#41; matrix of \(A_1, A_2\) and the &#39;stacked&#39; vector \(b_1, b_2\). Or, defining </p>
\[
\bar A \equiv 
\begin{bmatrix}
\sqrt{\lambda_1} A_1\\
\sqrt{\lambda_2} A_2
\end{bmatrix}
\]
<p>and</p>
\[
\bar b \equiv
\begin{bmatrix}
\sqrt{\lambda_1} b_1\\
\sqrt{\lambda_2} b_2
\end{bmatrix},
\]
<p>we have the equivalent problem</p>
\[
\min_x \,\, \lVert \bar A x - \bar b\lVert^2
\]
<p>which we can solve by the same means as before.</p>
<p>This &#39;extension&#39; now allows us to solve a large amount of problems &#40;even equality-constrained ones&#33; For example, say \(\lambda_i\) corresponds to an equality constraint, then we can send \(\lambda_i \to \infty\), which, if possible, sends that particular term to zero<a href="There are, of course, better ways of doing this which I&#39;ll present in some later post. For now, though, note that if the constraint is not achievable at equality, e.g. that we have &#36;\lVert Cx - d\lVert^2 \ge \varepsilon &gt; 0&#36; for any &#36;x&#36;, then the objective &#36;\lambda_i\lVert Cx - d\lVert^2 \ge \lambda_i\varepsilon \to \infty&#36; whenever we send &#36;\lambda_i \to \infty&#36;. This gives us a way of determining if the constraint is feasible &#40;which happens only if the program converges to a finite value for arbitrarily increasing &#36;\lambda_i&#36;&#41; or infeasible &#40;which happens, as shown before, if the minimization program diverges to infinity&#41;.">^eq-constraint</a>&#41;, including the image reconstruction problem that will be presented below. Yes, there are better methods, but I can&#39;t think of many that can be written in about 4 lines of Python with only a linear-algebra library &#40;not counting loading and saving, of course ðŸ˜‰&#41;.</p>
<h2 id="image_de-blurring_with_ls"><a href="#image_de-blurring_with_ls" class="header-anchor">Image de-blurring with LS</a></h2>
<p>Let&#39;s say we are given a blurred image, represented by some vector \(y\) with, say, a gaussian blur operator given by \(G\) &#40;which can be represented as a matrix&#41;. Usually, we&#39;d want to minimize a problem of the form</p>
\[
\min_x \,\,\lVert Gx - y \lVert^2
\]
<p>where \(x\) is the reconstructed image. In other words, we want the image \(x\) such that applying a gaussian blur \(G\) to \(x\) yields the closest possible image to \(y\). E.g. we really want something of the form</p>
\[
Gx \approx y.
\]
<p>Writing this out is a bit of a pain, but it&#39;s made a bit easier by noting that convolution with a 2D gaussian kernel is separable into two convolutions of 1D &#40;e.g. convolve the image with the up-down filter, and do the same with the left-right&#41; and by use of the Kronecker product to write out the individual matrices.<a href="Assuming we write out the image as a row-major order, with dimensions &#36;m\times m&#36;, we can write the &#91;Toeplitz matrix&#93;&#40;https://en.wikipedia.org/wiki/Toeplitz_matrix&#41; of the 1D gaussian convolution of length &#36;n&#36;, say &#36;T&#36;. Then the row-convolution of the image&#39;s one-dimensional vector representation is given by &#36;I_m \otimes T&#36;, where &#36;\otimes&#36; denotes the &#91;Kronecker product&#93;&#40;https://en.wikipedia.org/wiki/Kronecker_product&#41; of both matrices and &#36;I_m&#36; is the &#36;m\times m&#36; identity matrix. Similarly, we can do this for the column convolution by writing it out as &#36;T\otimes I_&#123;m&#43;n&#125;&#36; &#40;note the additional &#36;n&#36; which comes from the convolution&#33;&#41;, then the final &#36;G&#36; matrix is">^kronecker-conv</a> The final \(G\) is therefore the product of each of the convolutions. Just to show the comparison, here&#39;s the initial image, taken from <a href="https://commons.wikimedia.org/wiki/File:Lichtenstein_img_processing_test.png">Wikipedia</a></p>
<p>&lt;img src&#61;&quot;/images/constrained-ls-intro/initial_image.png&quot; class&#61;&quot;insert&quot;&gt; <em>Original greyscale image</em></p>
<p>and here&#39;s the image, blurred with a 2D gaussian kernel of size 5, with \(\sigma = 3\)</p>
<p>&lt;img src&#61;&quot;/images/constrained-ls-intro/blurred_image.png&quot; class&#61;&quot;insert&quot;&gt; <em>Blurred greyscale image. The vignetting comes from edge effects.</em></p>
<p>The kernel, for context, looks like:</p>
<p>&lt;img src&#61;&quot;/images/constrained-ls-intro/gaussian_kernel.png&quot; class&#61;&quot;insert&quot;&gt; <em>2D Gaussian Kernel with \(N=5, \sigma=3\)</em></p>
<p>Solving the problem, as given before, yields the final &#40;almost identical&#41; image:</p>
<p>&lt;img src&#61;&quot;/images/constrained-ls-intro/reconstructed_image.png&quot; class&#61;&quot;insert&quot;&gt; <em>The magical reconstructed image&#33;</em></p>
<p>Which was nothing but solving a simple least squares problem &#40;as we saw above&#41;&#33;</p>
<p>Now, you might say, &quot;why are we going through all of this trouble to write this problem as a least-squares problem, when we can just take the FFT of the image and the gaussian and divide the former by the latter? Isn&#39;t convolution just multiplication in the Fourier domain?&quot;</p>
<p>And I would usually agree&#33;</p>
<p>Except for one problem: while we may <em>know</em> the gaussian blurring operator on artificial images that <em>we</em> actively blur, the blurring operator that we provide for real images may not be fully representative of what&#39;s really happening&#33; By that I mean, if the real blurring operator is given by \(G^\text{real}\), it could be that our guess \(G\) is far away from \(G^\text{real}\), perhaps because of some non-linear effects, or random noise, or whatever.</p>
<p>That is, we know what photos, in general, look like: they&#39;re usually pretty smooth and have relatively few edges. In other words, the variations and displacements aren&#39;t large almost everywhere in most images. This is where the multi-objective form of least-squares comes in handyâ€”we can add a secondary &#40;or third, etc&#41; objective that allows us to specify how smooth the actual image should be&#33; </p>
<p>How do we do this, you ask? Well, let&#39;s consider the gradient at every point. If the gradient is large, then we&#39;ve met an edge since there&#39;s a large color variation between one pixel and its neighbour, similarly, if the gradient is small at that point, the image is relatively smooth at that point.</p>
<p>So, how about specifying that the sum of the norms of the gradients at every point be small?<a href="It turns out this has deep connections to a bunch of beautiful mathematical theories &#40;most notably, the theory of heat diffusion on manifolds&#41;, but we won&#39;t go into them here since it&#39;s relatively out of scope, though I may cover it in a later post.">^heat-diffusion</a> That is, we want the gradients to <em>all</em> be relatively small &#40;minus those at edges, of course&#33;&#41;, with some parameter that we can tune. In other words, let \(D_x\) be the difference matrix between pixel \((i,j)\) and pixel \((i+1,j)\) &#40;e.g. if our image is \(X\) then \((D_x X)_{ij} = X_{i+1,j} - X_{ij}\), and, similarly, let \(D_y\) be the difference matrix between pixel \((i, j)\) and \((i,j+1)\).<a href="These are slightly tricker to form for 2D images, but, as with the previous, we make heavy use of the Kronecker product. The derivative matrix for a 1D &#36;m&#36;-vector is the one formed by &#40;let&#39;s call it &#36;L&#36; with dimensions &#36;&#40;m-1&#41;\times m&#36;&#41;">^derivative-mat</a> Then our final objective is of the form</p>
\[
\min_x \,\,\lVert Gx - y \lVert^2 + \lambda\left(\lVert D_x x \lVert^2 + \lVert D_y x \lVert^2\right)
\]
<p>where \(\lambda \ge 0\) is our &#39;smoothness&#39; parameter. Note that, if we send \(\lambda \to \infty\) then we really care that our image is &#39;infinitely smooth&#39; &#40;what would that look like?<a href="You probably guessed it: it&#39;s just an image that is all the same color.">^smooth-image</a>&#41;, while if we send it to zero, we care that the reconstruction from the &#40;possibly not great&#41; approximation of \(G^\text{real}\) is really good. Now, let&#39;s compare the two methods with a slightly corrupted image:</p>
<p>&lt;img src&#61;&quot;/images/constrained-ls-intro/corrupted<em>blurred</em>image.png&quot; class&#61;&quot;insert&quot;&gt; <em>The corrupted, blurred image we feed into the algorithm</em></p>
<p>&lt;img src&#61;&quot;/images/constrained-ls-intro/initial_image.png&quot; class&#61;&quot;insert&quot;&gt; <em>Original greyscale image &#40;again, for comparison&#41;</em></p>
<p>&lt;img src&#61;&quot;/images/constrained-ls-intro/smoothed<em>corrupted</em>reconstructed<em>image</em>l&#61;1e-07.png&quot; class&#61;&quot;insert&quot;&gt; <em>Reconstruction with \(\lambda = 10^{-7}\)</em></p>
<p>&lt;img src&#61;&quot;/images/constrained-ls-intro/corrupted<em>reconstructed</em>image.png&quot; class&#61;&quot;insert&quot;&gt; <em>Reconstruction with original method</em></p>
<p>Though the normalized one has slightly larger grains, note that, unlike the original, the contrast isn&#39;t as heavily lost and the edges, etc, are quite a bit sharper.</p>
<p>We can also toy a bit with the parameter, to get some intuition as to what all happens:</p>
<p>&lt;img src&#61;&quot;/images/constrained-ls-intro/smoothed<em>corrupted</em>reconstructed<em>image</em>l&#61;0.001.png&quot; class&#61;&quot;insert&quot;&gt; <em>Reconstruction with \(\lambda = 10^{-3}\)</em></p>
<p>&lt;img src&#61;&quot;/images/constrained-ls-intro/smoothed<em>corrupted</em>reconstructed<em>image</em>l&#61;1e-05.png&quot; class&#61;&quot;insert&quot;&gt; <em>Reconstruction with \(\lambda = 10^{-5}\)</em></p>
<p>&lt;img src&#61;&quot;/images/constrained-ls-intro/smoothed<em>corrupted</em>reconstructed<em>image</em>l&#61;1e-10.png&quot; class&#61;&quot;insert&quot;&gt; <em>Reconstruction with \(\lambda = 10^{-10}\)</em></p>
<p>Of course, as we make \(\lambda\) large, note that the image becomes quite blurry &#40;e.g. &#39;smoother&#39;&#41;, and as we send \(\lambda\) to be very small, we end up with the same solution as the original problem, since we&#39;re saying that we care very little about the smoothness and much more about the reconstruction approximation.</p>
<p>To that end, one could think of many more ways of characterizing a &#39;natural&#39; image &#40;say, if we know what some colors should look like, what is our usual contrast, etc.&#41;, all of which will yield successively better results, but I&#39;ll leave with saying that LS, though relatively simple, is quite a powerful method for many cases. In particular, I&#39;ll cover fully-constrained LS &#40;in a more theoretical post&#41; in the future, but with an application to path-planning.</p>
<p>Hopefully this was enough to convince you that even simple optimization methods are pretty useful&#33; But if I didn&#39;t do my job, maybe you&#39;ll have to read some future posts. ;&#41;</p>
<p>If you&#39;d like, the complete code for this post can be found <a href="https://github.com/guillean/guille.site/tree/master/notebooks/constrained-ls-intro">here</a>.</p>
<p>      A nice picture usually helps with this:     <img src="/images/constrained-ls-intro/bounding-hyperplanes.png" alt="Convex envelope approximation" /></p>
<p>Each of the hyperplanes &#40;which are taken at the open, red circles along the curve; the hyperplanes themselves denoted by gray lines&#41; always lies below the graph of the function &#40;the blue parabola&#41;. We can write this as
&#36;&#36;
f&#40;y&#41;\ge &#40;y-x&#41;^T&#40;\nabla f&#40;x&#41;&#41; &#43; f&#40;x&#41;
&#36;&#36;
for all &#36;x, y&#36;.

This is usually taken to be the *definition* of a convex function, so we&#39;ll take it as such here. Now, if the point &#36;x^0&#36; is a local minimum, we must have &#36;\nabla f&#40;x^0&#41; &#61; 0&#36;, this means that
&#36;&#36;
f&#40;y&#41; \ge &#40;y-x^0&#41;^T&#40;\underbrace&#123;\nabla f&#40;x^0&#41;&#125;_&#123;&#61;0&#125;&#41; &#43; f&#40;x^0&#41; &#61; &#40;y-x^0&#41;^T0 &#43; f&#40;x^0&#41; &#61; f&#40;x^0&#41;,
&#36;&#36;
for any &#36;y&#36;.</p>
<p>In other words, that</p>
\[
f(y) \ge f(x^0),
\]
<p>for any \(y\). But this is the definition of a global minimum since the point \(f(x^0)\) is less than any other value the function takes on&#33; So we&#39;ve proved the claim that any local minimum &#40;in fact, more strongly, that any point with vanishing derivative&#41; is immediately a global minimum for a convex function. This is what makes convex functions so nice&#33;</p>
<p>      &#36;         G &#61; &#40;T\otimes I<em>&#123;m&#43;n&#125;&#41;&#40;I</em>m \otimes T&#41;     &#36;</p>
<p>which is much simpler to compute than the horrible initial expression dealing with indices. Additionally, these expressions are sparse &#40;e.g. the first is block-diagonal&#41;, so we can exploit that to save on both memory and processing time. For more info, I&#39;d recommend looking at the code for this entry.</p>
<p>  </p>
\[
L=\begin{bmatrix}
1 & -1 & 0 & 0 & \dots\\
0 & 1 & -1 & 0 & \dots\\
0 & 0 & 1 & -1 & \dots\\
\vdots & \vdots & \vdots & \vdots & \ddots
\end{bmatrix}
\]
<p>this is relatively straightforward to form using an off-diagonal matrix. Note also that this matrix is quite sparse, which saves us from very large computation. Now, we want to apply this to each row &#40;column&#41; of the 2D image which is in row-major form. So, we follow the same idea as before and pre &#40;post&#41; multiply by the identity matrix. That is, for an \(m\times m\) image</p>
\[
D_x = I_m\otimes L
\]
<p>and</p>
\[
D_y = L\otimes I_m
\]
<div class="page-foot">
    Built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and <a href="https://julialang.org">Julia</a>.
</div>
</div><!-- CONTENT ENDS HERE -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
        <script src="/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
