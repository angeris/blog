<!doctype html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
     <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
    
    <link rel="stylesheet" href="/css/franklin.css">
    <link rel="stylesheet" href="/css/basic.css">
    <link rel="icon" href="/assets/favicon.png">
     <title>Proximal gradient for SVM</title>  
</head>
<body>
    <header>
<div class="blog-name"><a href="/">Longest Path Search</a></div>
<nav>
  <ul>
    <li><a href="https://angeris.github.io">About</a></li>
    <li><a href="https://github.com/angeris">Code</a></li>
    <li><a href="/feed.xml">RSS feed</a></li>
    <li><a href="https://c.xkcd.com/random/comic/">Fun</a></li>
  </ul>
</nav>
</header>

    
    
    <div class="franklin-content">
        <h1>Proximal gradient for SVM</h1>
        <p>Posted <strong>2017-12-22</strong></p>
    </div>
    
    <!-- Content appended here -->
    
<div class="franklin-content">
<p>For a class that&#39;s currently being written &#40;<em>ahem</em>, EE104&#41;, Prof. Boyd posed an interesting problem of writing a &#40;relatively general, but ideally simple&#41; proximal-gradient optimizer. The idea is that would act as a black-box way for students to plug in machine learning models of a specific form and have the optimizer do all of the hard work immediately, while appearing as transparent as possible.</p>
<p>This led to a rabbit hole of asking what loss functions should be included in the optimization package &#40;huber, square, \(\ell_1\), etc.&#41;, many of which are relatively straightforward to implement—except, of course, SVM. Almost all of the other cases have few problems in implementation, since computing their proximal gradient is a direct computation &#40;e.g. \(\ell_1\) corresponds immediately to a <a href="https://en.wikipedia.org/wiki/Proximal_gradient_methods_for_learning#Solving_for_&#37;7F&#39;&quot;<code>UNIQ--postMath-0000002A-QINU</code>&quot;&#39;&#37;7F_proximity_operator">shrinkage operator</a> as we&#39;ll see below&#41;, yet the SVM loss has a set of hard constraints which I haven&#39;t found a nice way of stuffing into the prox-gradient step &#40;and I suspect that there are no such nice ways, but I&#39;d love to be proven wrong, here&#41;; thus, every step requires finding a projection into a polygon, which is, itself, a second optimization problem that has to be solved.</p>
<p>Prox gradients are &#40;generally&#41; really well-behaved and I&#39;ve been having some fun trying to really understand how well they work as general optimizers—I write a few of those thoughts below along with an odd solution to the original problem.</p>
<h2 id="proximal_gradients"><a href="#proximal_gradients" class="header-anchor">Proximal gradients</a></h2>
<p>Proximal gradients are a nice idea emerging from convex analysis which provide useful ways of dealing with tricky, non-differentiable convex functions. In particular, you can think of the proximal gradient of a given function as an optimization problem that penalizes taking steps &quot;too far&quot; in a given direction. Better yet &#40;and perhaps one of the main useful points&#41; is that most functions we care about have relatively nice proximal gradient operators&#33;</p>
<p>Anyways, for now, let&#39;s define the proximal gradient of a function \(g\) at some point \(x\) &#40;usually denoted \(\text{prox}_g(x)\), though I will simply call it \(P_g(x)\) for shorter notation&#41; to be</p>
\[
P_g(x) \equiv \mathop{\arg\!\min}\limits_y \left(g(y) + \frac{1}{2}\lVert x- y\lVert_2^2\right)
\]
<p>the definition is useful only because, if we allow \(\partial g(u)\) to be the subdifferential of \(g\) at \(u\), then optimality guarantees that, if \(y = P_g(x)\), then &#40;by knowing that the subdifferential must be zero at the optimum&#41; we have</p>
\[
x - y \in \partial g(y).
\]
<p>In other words, \(0 \in \partial g(y)\) iff \(y\) is a fixed point of \(P_g\)—that is, we have reached a minimizer of \(g\) iff</p>
\[
y = P_g(y).
\]
<p>Additionally, there&#39;s no weird trickery that has to be done with subdifferentials since the result of \(P_g\) is always unique, which is a nice side-benefit. Using just this, we can already begin to do some optimization. For example, let&#39;s consider the &#40;somewhat boring, but enlightening&#41; example of minimization of the \(\ell_1\)-norm. Using the fact that</p>
\[
u = P_{\lambda |\cdot|}(x) \iff x - u \in \partial |u|
\]
<p>and using the fact that the \(\ell_1\)-norm is separable, we have, whenever \(u>0\) &#40;I&#39;m considering a single term of the sum, here&#41;</p>
\[
x - u = \lambda \implies u = x-\lambda\text{ whenever } x-\lambda > 0
\]
<p>similarly for the \(u=0\) case we have &#40;where \(\lambda S\) for some set \(S\) is just multiplication of every element in the set by \(\lambda\)&#41;</p>
\[
x - u = x \in \lambda [-1, 1] = [-\lambda, \lambda].
\]
<p>that is</p>
\[
u = 0\text{ whenever } |x|\le \lambda
\]
<p>and similarly for the \(u<0\) case, we have \(u = x + \lambda\) if \(x < -\lambda\). Since this is done for each component, the final operator has action</p>
\[
u_i = \begin{cases}
x_i - \lambda, & x_i > \lambda\\
0, & |x_i| \le \lambda \\
x_i + \lambda, & x_i < -\lambda.
\end{cases}
\]
<p>This operator is called the &#39;shrinkage&#39; operator because of its action on its input: if \(x_i\) is greater than our given \(\lambda\), then we shrink it by that amount. Note then, that successively applying &#40;in the same manner as SGD&#41; the update rule</p>
\[
u^{i+1} = P_{|\cdot|}(u^i)
\]
<p>correctly yields the minimum of the given convex function, i.e. 0. Of course, this isn&#39;t particularly surprising since we already know how to optimize the \(\ell_1\)-norm function, \(\lVert x \lVert_1\) &#40;just set it to zero&#33;&#41;, but it will help out quite a bit when considering more complicated functions.</p>
<h2 id="proximal_gradient_update"><a href="#proximal_gradient_update" class="header-anchor">Proximal gradient update</a></h2>
<p>Now, given a problem of the form</p>
\[
\min_x f(x) + g(x)
\]
<p>where \(f\) is differentiable, and \(g\) is convex, we can write down a possible update in the same vein as the above, except that we now also update our objective for \(f\)<em>and</em> \(g\) at the same time</p>
\[
u^{i+1} = P_{\gamma^{i+1} g}(u^i - \gamma^{i+1}\nabla f(u^i)).
\]
<p>Here, \(\gamma^i\) is defined to be the step size for step \(i\). It turns out we can prove several things about this update, but, perhaps most importantly, we can show that it works.</p>
<p>Anyways, this is all I&#39;ll say about the proximal gradient step as there are several good resources on the proximal gradient method around which will do a much better job of explaining it than I probably ever will: see <a href="https://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf">this</a> for example.</p>
<h2 id="optimizing_svm_using_proximal_gradient"><a href="#optimizing_svm_using_proximal_gradient" class="header-anchor">Optimizing SVM using proximal gradient</a></h2>
<h3 id="initial_problem"><a href="#initial_problem" class="header-anchor">Initial problem</a></h3>
<p>I assume some familiarity with SVMs, but the program given might require a bit of explanation. The idea of an SVM is as a soft-margin classifier &#40;there are hard-magin SVMs, but we&#39;ll consider the former variety for now&#41;: we penalize the error of being on the wrong side of the decision boundary in a linear form &#40;with zero penalty for being on the correct side of the decision boundary&#41;. The only additional thing is that we also require that the margin&#39;s size also be penalized such that it doesn&#39;t depend overly on a particular variable &#40;e.g. as a form of regularization&#41;.</p>
<p>The usual quadratic program for an SVM is, where \(\xi^\pm_i\) are the slack variables indicating how much the given margin is violated, \(\varepsilon > 0\) is some arbitrary positive constant, and \(\mu\) is the hyperplane and constant offset found by the SVM &#40;e.g. by allowing the first feature of a positive/negative sample to be \((x^\pm_i)_0 = 1\)&#41;:</p>
\[
\begin{aligned}
& \underset{\xi, \mu}{\text{minimize}}
& & \sum_i \xi^+_i + \sum_i \xi^-_i + C\lVert \mu \lVert_2 \\
& \text{subject to}
& & \mu^Tx^+_i - \varepsilon \ge -\xi^+_i,\,\,\text{ for all } i \\
&&& \mu^Tx^-_i + \varepsilon \le \xi^-_i,\,\,\text{ for all } i\\
&&& \xi^\pm_i \ge 0,\,\,\text{ for all } i
\end{aligned}
\]
<p>we can rerwite this immediately, given that the class that our data point \(i\) corresponds to is \(y^{i}\in \\{-1, +1\\}\) to a much nicer form</p>
\[
\begin{aligned}
& \underset{\xi, \mu}{\text{minimize}}
& & 1^T \xi + C\lVert \mu \lVert_2 \\
& \text{subject to}
& & -y^i \mu^Tx_i - \varepsilon \ge -\xi_i,\,\,\text{ for all } i \\
&&& \xi_i \ge 0,\,\,\text{ for all } i
\end{aligned}
\]
<p>and noting that the objective is homogeneous of degree one, we can just multiply the constraints and all variables by \(\frac{1}{\varepsilon}\) which yields the immediate result &#40;after flipping some signs and inequalities&#41;</p>
\[
\begin{aligned}
& \underset{\xi, \mu}{\text{minimize}}
& & 1^T\xi + C\lVert \mu \lVert_2 \\
& \text{subject to}
& & \xi_i\ge y^i \mu^Tx_i +1,\,\,\text{ for all } i \\
&&& \xi_i \ge 0,\,\,\text{ for all } i
\end{aligned}
\]
<p>which, after changing the \(\ell_2\) regularizer to an \(\ell_2^2\)-norm regularizer &#40;which is equivalent for approriate regularization hyperparameter, say \(\eta\)<sup id="fnref:hyperparameter"><a href="#fndef:hyperparameter" class="fnref">[1]</a></sup>&#41; yields</p>
\[
\begin{aligned}
& \underset{\xi, \mu}{\text{minimize}}
& & 1^T \xi + C\lVert \mu \lVert_2^2 \\
& \text{subject to}
& & \xi_i\ge y^i \mu^Tx_i +1,\,\,\text{ for all } i \\
&&& \xi_i \ge 0,\,\,\text{ for all } i.
\end{aligned}
\]
<p>This is the final program we care about and the one we have to solve using our proximal gradient operator. In general, it&#39;s not obvious how to fit the inequalities into a step, so we have to define a few more things.</p>
<h3 id="set_indicators"><a href="#set_indicators" class="header-anchor">Set indicators</a></h3>
<p>For now, let&#39;s define the set indicator function</p>
\[
g_S(x) = \begin{cases}
0 & x\in S\\
+\infty & x\not\in S
\end{cases}
\]
<p>which is convex whenever \(S\) is convex; we can use this to encode the above constraints &#40;I drop the \(S\) for convenience in what follows&#41; such that a program equivalent to be above is</p>
\[
\underset{\xi, \mu}{\text{minimize}} \,\, 1^T \xi + C\lVert \mu \lVert_2^2 + g(\mu, \xi)
\]
<p>which is exactly what we wanted&#33; Why? Well:</p>
\[
\underset{\xi, \mu}{\text{minimize}}\,\, \underbrace{1^T \xi + C\lVert \mu \lVert_2^2}_\text{differentiable} + \underbrace{g(\mu, \xi)}_\text{convex}
\]
<p>so now, we just need to find the proximal gradient operator for \(g(x)\) &#40;which is not as nice as one would immediately think, but it&#39;s not bad&#33;&#41;.</p>
<h3 id="proximal_gradient_of_the_linear_inequality_indicator"><a href="#proximal_gradient_of_the_linear_inequality_indicator" class="header-anchor">Proximal gradient of the linear inequality indicator</a></h3>
<p>Now, let&#39;s generalize the problem a bit: we&#39;re tasked with the question of finding the prox-gradient of \(g_S(x)\) such that \(S\) is given by some set of inequalities \(S = \{x\,|\, Ax\le b\}\) for some given \(A, b\).<sup id="fnref:equivalence"><a href="#fndef:equivalence" class="fnref">[2]</a></sup> That is, we require</p>
\[
\mathop{\arg\!\min}\limits_y \frac{1}{2\lambda}\lVert x- y\lVert_2^2 + g_S(y)
\]
<p>which can be rewritten as the equivalent program &#40;where the \(1/2\lambda\) is dropped since it&#39;s just a proportionality constant&#41;</p>
\[
\begin{aligned}
& \underset{y}{\text{minimize}}
& & \lVert x- y\lVert_2^2 \\
& \text{subject to}
& & Ay\le b
\end{aligned}
\]
<p>it turns out this program isn&#39;t nicely solvable using the prox-gradient operator &#40;since there&#39;s no obvious way of projecting onto \(Ax\le b\) and <em>also</em> minimizing the quadratic objective&#41;. But, of course, I wouldn&#39;t be writing this if there wasn&#39;t a cute trick or two we could do: note that this program has a strong dual &#40;i.e. the values of the <a href="https://en.wikipedia.org/wiki/Duality_&#40;optimization&#41;">dual program</a> and the primal are equal&#41; by <a href="https://en.wikipedia.org/wiki/Slater&#37;27s_condition">Slater&#39;s condition</a>, so how about trying to solve the dual program? The lagrangian is</p>
\[
\mathcal{L} = \lVert x- y\lVert_2^2 + \gamma^T(Ay - b)
\]
<p>from which we can derive the dual by taking derivatives over \(y\):</p>
\[
\nabla\mathcal{L} = y-x + A^T\gamma = 0 \implies y = x - A^T\gamma
\]
<p>and plugging in the above &#40;and simplifying&#41; yields the program</p>
\[
\begin{aligned}
& \underset{\eta}{\text{maximize}}
& & \eta^T(Ax-b) - \frac{1}{2}\lVert A^T\eta\lVert_2^2 \\
& \text{subject to}
& & \eta \ge 0
\end{aligned}
\]
<p>from which we can reconstruct the original solution by the above, given:</p>
\[
y = x - A^T\eta.
\]
<p>This program now has a nice prox step, since \(\left(P_{\eta \ge 0}(\eta)\right)_i = \max\{0, \eta_i\}\) &#40;the &#39;positive&#39; part of \(\eta_i\), in other words&#41;. This latter case is left as an exercise for the reader.</p>
<h2 id="final_thoughts"><a href="#final_thoughts" class="header-anchor">Final Thoughts</a></h2>
<p>Putting the above together yields a complete way of optimizing the SVM program: first, take a single step of the initial objective, then find the minimum projection on the polygon over the given inequality constraints by using the second method, and then take a new step on the original, initial program presented.</p>
<p>Of course, this serves as little more than an academic exercise &#40;I&#39;m not sure how thrilled either Boyd nor Lall would be at using dual programs in 104, even in a quasi-black-box optimizer&#41;, but it may be of interest to people who take interest in relatively uninteresting things &#40;e.g. me&#41; or to people who happen to have <em>really freaking fast</em> proximal gradient optimizers &#40;not sure these exist, but we can pretend&#41;.</p>
<p><table class="fndef" id="fndef:hyperparameter">
    <tr>
        <td class="fndef-backref"><a href="#fnref:hyperparameter">[1]</a></td>
        <td class="fndef-content">We&#39;re also finding this by using cross-validation, rather than a-priori, so it doesn&#39;t matter too much.</td>
    </tr>
</table>
 <table class="fndef" id="fndef:equivalence">
    <tr>
        <td class="fndef-backref"><a href="#fnref:equivalence">[2]</a></td>
        <td class="fndef-content">Note that this is equivalent to the original problem, actually. The forwards direction is an immediate generalization. The backwards direction is a little more difficult, but can be made simple by considering, say, only positive examples.</td>
    </tr>
</table>
</p>
<div class="page-foot">
    Built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and <a href="https://julialang.org">Julia</a>.
</div>
</div><!-- CONTENT ENDS HERE -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
  </body>
</html>
