<!doctype html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
     <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
    
    <link rel="stylesheet" href="/css/franklin.css">
    <link rel="stylesheet" href="/css/basic.css">
    <link rel="icon" href="/assets/favicon.png">
     <title>A non-counting lower bound for the expected distance of a simple random walk</title>  
</head>
<body>
    <header>
<div class="blog-name"><a href="/">Longest Path Search</a></div>
<nav>
  <ul>
    <li><a href="https://angeris.github.io">About</a></li>
    <li><a href="https://github.com/angeris">Code</a></li>
    <li><a href="/feed.xml">RSS feed</a></li>
    <li><a href="https://c.xkcd.com/random/comic/">Fun</a></li>
  </ul>
</nav>
</header>

    
    
    <div class="franklin-content">
        <h1>A non-counting lower bound for the expected distance of a simple random walk</h1>
        <p>Posted <strong>2023-06-20</strong></p>
    </div>
    
    <!-- Content appended here -->
    
<div class="franklin-content">
<p>It&#39;s been a while since I&#39;ve updated the blog &#40;likely due to the fact that I&#39;ve been struggling to get it to work with Github pages...&#41;. Anyways, it&#39;ll, at some point, be migrated over, but for now this will have to do.</p>
<p>This post will focus on a particular, nearly silly, proof of a lower bound for the distance of an unbiased random walk, defined as</p>
\[
X = \sum_{i=1}^n X_i,
\]
<p>where \(X_i \sim \{\pm 1\}\), uniformly. The quantity we want to find a lower bound to is</p>
\[
\mathbf{E}[|X|],
\]
<p>as \(n\) is large. We know from a basic, if somewhat annoying, counting argument that</p>
\[
\mathbf{E}[|X|] \sim \sqrt{\frac{2}{\pi}}\sqrt{n},
\]
<p>when \(n \gg 1\). In general, we&#39;re interested in bounds of the form</p>
\[
\mathbf{E}[|X|] \ge \Omega(\sqrt{n}).
\]
<p>Bounds like these are applicable in a number of important lower bounds for online convex optimization &#40;see, <em>e.g.</em>, Hazan&#39;s <a href="https://arxiv.org/abs/1909.05207">lovely overview</a>, section 3.2&#41; though we won&#39;t be talking too much about the applications on this one.</p>
<p>Additionally, since \(\mathbf{E}[X^2] = n\) &#40;which follows by expanding and using the fact that \(X_i\) are independent with mean zero&#41; then</p>
\[
    \mathbf{E}[|X|] \le \sqrt{\mathbf{E}[X^2]} = \sqrt{n},
\]
<p>so we know that this bound is tight up to a constant. The first inequality here follows from an application of Jensen&#39;s inequality to the square root function &#40;which is concave&#41;.</p>
<h2 id="why_a_non-counting_proof"><a href="#why_a_non-counting_proof" class="header-anchor">Why a non-counting proof?</a></h2>
<p>Mostly because I&#39;m bad at counting and always end up with a hilarious number of errors. Plus, this proof is easily generalizable to a number of other similar results&#33;</p>
<h2 id="proof_idea"><a href="#proof_idea" class="header-anchor">Proof idea</a></h2>
<p>One simple method for lower-bounding the expectation of a variable like \(|X|\) is to note that \(|X|\) is nonnegative, so we have the following &#39;silly&#39; bound</p>
\[

\mathbf{E}[|X|] \ge \mathbf{E}[a\mathbf{1}_{|X| \ge a}] = a \mathbf{Pr}(|X| \ge a),
\]
<p>for any \(a \ge 0\), where \(\mathbf{1}_{|X| \ge a}\) is the indicator function for the event \(|X| \ge a\), that is 1 if \(|X| \ge a\) and zero otherwise. &#40;The bound follows from the fact that \(|X| \ge a \mathbf{1}_{|X|\ge a}\) pointwise.&#41; Maximizing over \(a\), assuming we have a somewhat tight lower bound over the probability that \(|X| \ge a\), then this approach might give us a reasonable lower bound.</p>
<p>In a very general sense, we want to show that \(|X|\) is &#39;anticoncentrated&#39;; <em>i.e.</em>, it is reasonably &#39;spread out&#39;, which would indicate that its expectation cannot be too small, since it is nonnegative.</p>
<h2 id="attempt_1"><a href="#attempt_1" class="header-anchor">Attempt #1</a></h2>
<p>The first idea &#40;or, at least, my first idea&#41; would be to note that, since \(\mathbf{E}[X^2]\) is on the order of \(n\), then maybe we can use this fact to construct a bound for \(\mathbf{E}[|X|]\) which &#39;should be&#39; on the order of \(\sqrt{n}\) assuming some niceness conditions, for example, that \(|X| \le n\) is a bounded variable.</p>
<p>Unfortunately, just these two simple facts are not enough to prove the claim&#33; We can construct a nonnegative random variable \(Y\ge 0\) such that its second moment is \(\mathbf{E}[Y^2] = n\), it is bounded by \(Y \le n\), yet \(\mathbf{E}[Y] = 1\). In other words, we wish to construct a variable that is very concentrated around \(0\), with &#39;sharp&#39; peaks at larger values.</p>
<p>Of course, the simplest example would be to take \(Y = n\) with probability \(1/n\) and \(Y=0\) with probability \(1-1/n\). Clearly, this variable is bounded, and has \(n\) as its second moment. On the other hand,</p>
\[
\mathbf{E}[Y] = (1/n)n + (1-1/n)0 = 1,
\]
<p>which means that the best bound we can hope for, using just these conditions &#40;nonnegativity, boundedness, and second moment bound&#41; on a variable, is a constant. &#40;Indeed, applying a basic argument, we find that this is the smallest expectation possible.&#41;</p>
<p>This suggests that we need a little more control over the tails of \(|X|\), which gets us to...</p>
<h2 id="attempt_2_and_solution"><a href="#attempt_2_and_solution" class="header-anchor">Attempt #2 &#40;and solution&#41;</a></h2>
<p>Another easy quantity to compute in this case is \(\mathbf{E}[X^4]\). &#40;And, really, any even power of \(X\) is easy. On the other hand, since \(X\) has a distribution that is symmetric around 0, all odd moments are 0.&#41; Splitting the sum out into each of the possible quartic terms, we find that any term containing an odd power of \(X_i\) will be zero in expectation as the \(X_i\) are independent. So, we find</p>
\[
\mathbf{E}[X^4] = \sum_{i} \mathbf{E}[X_i^4] + \sum_{i\ne j} \mathbf{E}[X_i^2X_j^2] = n + n(n-1) = n^2.
\]
<p>This quantity will come in handy soon.</p>
<p>We can, on the other hand, split up the expectation of \(X^2\) in a variety of ways. One is particularly handy to get a tail <em>lower bound</em> like the one we wanted in our proof idea &#40;above&#41;:</p>
\[
\mathbf{E}[X^2] = \mathbf{E}[X^2\mathbf{1}_{|X| < a}] + \mathbf{E}[X^2\mathbf{1}_{|X| \ge a}] \le a^2 + \mathbf{E}[X^2\mathbf{1}_{|X| \ge a}].
\]
<p>The latter term can be upper bounded using Cauchy–Schwarz,<sup id="fnref:csproof"><a href="#fndef:csproof" class="fnref">[1]</a></sup></p>
\[
\mathbf{E}[X^2\mathbf{1}_{|X| \ge a}] \le \sqrt{\mathbf{E}[X^4]}\sqrt{\mathbf{E}[\mathbf{1}_{|X| \ge a}]}.
\]
<p>&#40;Since \(\mathbf{1}_{|X| \ge a}^2 = \mathbf{1}_{|X| \ge a}\).&#41; And, since \(\mathbf{E}[\mathbf{1}_{|X| \ge a}] = \mathbf{Pr}(|X| \ge a)\), we finally have:</p>
\[
\mathbf{E}[X^2] \le a^2 + \sqrt{\mathbf{E}[X^4]}\sqrt{\mathbf{Pr}(|X| \ge a)}.
\]
<p>Rearranging gives us the desired lower bound,</p>
\[
\mathbf{Pr}(|X| \ge a) \ge \frac{(\mathbf{E}[X^2] - a^2)^2}{\mathbf{E}[X^4]}.
\]
<p>&#40;This is a Paley–Zygmund-style bound, except over \(X^2\) rather than nonnegative \(X\).&#41;</p>
<p>Now, since we know that</p>
\[
\mathbf{E}[|X|] \ge a \mathbf{Pr}(|X| \ge a),
\]
<p>then we have</p>
\[
\mathbf{E}[|X|] \ge a \frac{(\mathbf{E}[X^2] - a^2)^2}{\mathbf{E}[X^4]}.
\]
<p>Parametrizing \(a\) by \(a = \alpha\sqrt{\mathbf{E}[X^2]}\) for some \(0 \le \alpha \le 1\), we then have</p>
\[
\mathbf{E}[|X|] \ge \alpha(1-\alpha^2)^2\frac{\mathbf{E}[X^2]^{3/2}}{\mathbf{E}[X^4]}.
\]
<p>The right-hand-side is maximized at \(\alpha = 1/\sqrt{5}\), which gives the following lower bound</p>
\[
\mathbf{E}[|X|] \ge \frac{16}{25\sqrt{5}}\frac{\mathbf{E}[X^2]^{3/2}}{\mathbf{E}[X^4]}.
\]
<p>And, finally, using the fact that \(\mathbf{E}[X^2] = n\) and \(\mathbf{E}[X^4] = n^2\), we get the final result:</p>
\[
\mathbf{E}[|X|] \ge \frac{16}{25\sqrt{5}}\sqrt{n} \ge \Omega(\sqrt{n}),
\]
<p>as required, with no need for combinatorics&#33; Of course the factor of \(16/(25\sqrt{5})
\approx .29\) is rather weak compared to the factor of \(\sqrt{2/\pi} \approx
.80\), but this is ok for our purposes.</p>
<h2 id="general_extensions"><a href="#general_extensions" class="header-anchor">General extensions</a></h2>
<p>Of course, similar constructions also hold rather nicely for things like uniform \([-1, 1]\) variables, or Normally distributed, mean zero variables. Any variable for which the second and fourth moment can be easily computed allows us to compute a lower bound on this expectation. &#40;Expectations of the absolute value of the sums of independently drawn versions of these variables could be similarly computed.&#41; These have no obvious combinatorial analogue, so those techinques cannot be easily generalized, whereas this bound applies immediately.</p>
<table class="fndef" id="fndef:csproof">
    <tr>
        <td class="fndef-backref"><a href="#fnref:csproof">[1]</a></td>
        <td class="fndef-content">Possibly the most elegant proof of Cauchy–Schwarz I know is based on minimizing a quadratic, and goes a little like this. Note that \(\mathbf{E}[(X - tY)^2]\ge 0\) for any \(t \in \mathbf{R}\). &#40;That this expectation exists can be shown for any \(t\) assuming both \(X\) and \(Y\) have finite second moment. If not, the inequality is also trivial.&#41; Expanding gives \(\mathbf{E}[X^2] - 2t\mathbf{E}[XY] + t^2\mathbf{E}[Y^2] \ge 0\). Minimizing the left hand side over \(t\) then shows that \(t^\star = \mathbf{E}[XY]/\mathbf{E}[Y^2]\), which gives </p>
\[ \mathbf{E}[X^2] - \frac{\mathbf{E}[XY]^2}{\mathbf{E}[Y^2]} \ge 0.\]
<p>Multiplying both sides by \(\mathbf{E}[Y^2]\) gives the final result.</td>
    </tr>
</table>

<div class="page-foot">
    Built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and <a href="https://julialang.org">Julia</a>.
</div>
</div><!-- CONTENT ENDS HERE -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
  </body>
</html>
